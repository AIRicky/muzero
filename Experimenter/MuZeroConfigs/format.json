{
  "name": "format",
  "algorithm": "Algorithm name @see experimenter Enum",

  "args": {
    "numIters": "(int) Number of iterations to repeat the training loop (self play - training - pitting)",
    "numEps": "(int) Number of episodes to perform self play for collecting training examples",
    "numTrainingSteps": "(int) Number of weight updates to perform in the backpropagation step",
    "tempThreshold": "(double) A boundary for the exploration temperature when exponentiating visit counts in MCTS",
    "dirichlet_alpha": "(double) Alpha parameter of the dirichlet distribution to sample noise from for exploration",
    "exploration_fraction": "(double: [0, 1]) Fraction to sample based on noise of the dirichlet for exploration vs the network prior",
    "maxlenOfQueue": "(int) Maximum number of game examples to train the neural networks on",
    "numMCTSSims": "(int) Number of planning moves for MCTS to simulate",
    "prioritize": "(bool) Set to true when using prioritized sampling from the replay buffer (used in Atari)",
    "prioritize_alpha": "(double) Exponentiation factor for computing probabilities in prioritized replay",
    "prioritize_beta": "(double) Exponentiation factor for exponentiating the importance sampling ratio in prioritized replay",
    "K": "(int > 0) Number of 'hypothetical' steps to unroll the MuZero network for during backpropagation",
    "n_steps": "(int > 0) Amount of steps to look ahead for rewards before bootstrapping (value function estimation)",
    "c1": "(double) First exploration constant for MuZero in the PUCT formula",
    "c2": "(double) Second exploration constant for MuZero in the PUCT formula",
    "gamma": "(double: [0, 1]) MDP Discounting factor for future rewards",

    "minimum_reward": "(double) Lower bound on the cumulative reward of the environment (null if not known)",
    "maximum_reward": "(double) Upper bound on the cumulative reward of the environment (null if not known)",

    "checkpoint": "(path: string) Directory to store sampled experiences and fitted parameters in",
    "load_model": "(bool) Whether to load in a previously trained model from the checkpoint directory",
    "load_folder_file": "(Array, ['directory', 'file']) Specify which model to load in",
    "numItersForTrainExamplesHistory": "(int) Maximum number of self play iterations kept in the deque"
  },

  "net_args": {
    "lr": "(double) Learning rate for the neural network's optimizer",
    "l2": "(double) Penalty scalar for l2 loss of network weights",
    "dropout": "(double) DropOut regularization rate for the neural network",
    "batch_size": "(int) Stochastic gradient descent batch size",
    "cuda": "(bool) Whether to make use of GPU accelerated computation (NVIDIA only)",
    "num_channels": "(int) Number of feature maps in the convolutional network",
    "num_towers": "(int) Number of convolutional towers to place in each network",
    "len_dense": "(int) Number of fully connected layers to use as the head of the CNN",
    "size_dense": "(int) Number of hidden nodes to use in the fully connected layers",
    "support_size": "(int) Number of integers for the reward distribution (defaults to a Gaussian if set to 0)",
    "observation_length": "(int) Number of observations in the history to be passed into the encoder"
  }
}
